{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Ryan Ku\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "import time\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# locations that aren't counties but can be treated as one for these purposes\n",
    "# EXCEPTIONS = [\"District of Columbia\"]\n",
    "EXCEPTIONS = []\n",
    "\n",
    "# returns a partial dataframe filtered by col:text (key:value) pair in filter_dict (i.e. removes all rows that don't satisfy filter_dict)\n",
    "# if remove_dict is specified, this function removes rows that satisfy remove_dict, BUT keeps any that also satisfies filter_dict\n",
    "def filter_by_value(df, filter_dict: dict = {}, remove_dict: dict = {}):\n",
    "    # if remove_dict was specified, remove rows that satisfy remove_dict BUT not filter_dict (i.e. also keep everything satisfied by filter_dict):\n",
    "    i_remove = set()\n",
    "    i_keep = set()\n",
    "    if len(remove_dict) > 0:\n",
    "        for col, text in remove_dict.items():\n",
    "            if text == \"NaN\":\n",
    "                i_remove = i_remove.union(set(df[df[col].isna()].index))\n",
    "            else:\n",
    "                # if is list instead, run through list\n",
    "                if type(text) is list:\n",
    "                    for item in text:\n",
    "                        # if is int/float instead, check integer value\n",
    "                        if type(item) is int or type(item) is float:\n",
    "                            i_remove = i_remove.union(set(df[df[col] == item].index))\n",
    "                        else:\n",
    "                            i_remove = i_remove.union(set(df[df[col].str.contains(item)].index))\n",
    "                # if is int/float instead, check integer value\n",
    "                elif type(text) is int or type(text) is float:\n",
    "                    i_remove = i_remove.union(set(df[df[col] == text].index))\n",
    "                else:\n",
    "                    i_remove = i_remove.union(set(df[df[col].str.contains(text)].index))\n",
    "        for col, text in filter_dict.items():\n",
    "            if text == \"NaN\":\n",
    "                i_keep = i_keep.union(set(df[df[col].isna()].index))\n",
    "            else:\n",
    "                # if is list instead, run through list\n",
    "                if type(text) is list:\n",
    "                    for item in text:\n",
    "                        # if is int/float instead, check integer value\n",
    "                        if type(item) is int or type(item) is float:\n",
    "                            i_keep = i_keep.union(set(df[df[col] == item].index))\n",
    "                        else:\n",
    "                            i_keep = i_keep.union(set(df[df[col].str.contains(item)].index))\n",
    "                        \n",
    "                # if is int/float instead, check integer value\n",
    "                elif type(text) is int or type(text) is float:\n",
    "                    i_keep = i_keep.union(set(df[df[col] == text].index))\n",
    "                else:\n",
    "                    i_keep = i_keep.union(set(df[df[col].str.contains(text)].index))\n",
    "        return df.drop(index = i_remove - i_keep)\n",
    "    # if remove_dict was not specified, filter rows by filter_dict:\n",
    "    else:\n",
    "        data = df.copy()\n",
    "        for col, text in filter_dict.items():\n",
    "            if text == \"NaN\":\n",
    "                i_remove = i_remove.union(set(data[~data[col].isna()].index))\n",
    "            else:\n",
    "                # if is list instead, run through list\n",
    "                if type(text) is list:\n",
    "                    for item in text:\n",
    "                        # if is int/float instead, check integer value\n",
    "                        if type(item) is int or type(item) is float:\n",
    "                            i_keep = i_keep.union(set(data[data[col] == item].index))\n",
    "                        else:\n",
    "                            i_keep = i_keep.union(set(data[data[col].str.contains(item)].index))\n",
    "                        \n",
    "                # if is int/float instead, check integer value\n",
    "                elif type(text) is int or type(text) is float:\n",
    "                    i_remove = i_remove.union(set(data[~(data[col] == text)].index))\n",
    "                else:\n",
    "                    i_remove = i_remove.union(set(data[~data[col].str.contains(text)].index))\n",
    "\n",
    "            data.drop(index=i_remove, inplace=True)\n",
    "            i_remove = set()\n",
    "        if len(i_keep) > 0:\n",
    "            return data.drop(index = set(data.index) - i_keep)\n",
    "        return data\n",
    "\n",
    "# removes all rows that don't correspond to individual counties (only works on USDoA datasets)\n",
    "# NOTE: boroughs are ignored for sake of simplicity (there's only ~15-20 anyways)\n",
    "def filter_counties(df, exceptions: list = []):\n",
    "    # remove rows that don't contain the word \"County\" in the \"County\" column\n",
    "    if len(EXCEPTIONS) > 0:\n",
    "        i_remove = df[~df[\"County\"].str.contains(\"County\") & ~df[\"County\"].str.contains(\"|\".join(EXCEPTIONS))].index\n",
    "    else:\n",
    "        i_remove = df[~df[\"County\"].str.contains(\"County\")].index\n",
    "    return df.drop(index = i_remove)\n",
    "\n",
    "# return true if NAs found under specific column\n",
    "def any_na(df, col) -> bool:\n",
    "    return df[col].isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieves relevant virus data\n",
    "def get_virus_data():\n",
    "    dtype = {\n",
    "        \"fips\" : \"Int32\",\n",
    "        \"cases\" : \"Int32\",\n",
    "        \"deaths\" : \"Int32\"\n",
    "    }\n",
    "    columns = {\n",
    "        \"date\" : \"Date\",\n",
    "        \"county\" : \"County\",\n",
    "        \"state\" : \"State\",\n",
    "        \"fips\" : \"FIPS\",\n",
    "        \"cases\" : \"Cases\",\n",
    "        \"deaths\" : \"Deaths\"\n",
    "    }\n",
    "    data = pd.read_csv(\"./datasets/NYTimes/us-counties_2020_6_3.csv\", sep=\",\", header=0, engine=\"python\", dtype=dtype, parse_dates=[\"date\"]).rename(columns=columns)\n",
    "\n",
    "    # remove data from unknown locations, BUT keep Puerto Rico data\n",
    "    data = filter_by_value(data, filter_dict={\"State\": \"Puerto Rico\"}, remove_dict={\"County\": \"Unknown\"})\n",
    "    # add Day column to track number of days since start of data\n",
    "    data[\"Day\"] = (data[\"Date\"] - data[\"Date\"][0]).dt.days\n",
    "    # TODO: convert state to state abbreviation\n",
    "    data.fillna(value={\"FIPS\": -1}, inplace=True)\n",
    "    return data\n",
    "\n",
    "# retrieves relevant education data\n",
    "def get_education_data():\n",
    "    dtype = {\n",
    "        \"FIPS Code\" : \"Int32\",\n",
    "        \"Less than a high school diploma, 2014-18\" : \"Int32\",\n",
    "        \"High school diploma only, 2014-18\" : \"Int32\",\n",
    "        \"Some college or associate's degree, 2014-18\" : \"Int32\",\n",
    "        \"Bachelor's degree or higher, 2014-18\" : \"Int32\",\n",
    "    }\n",
    "    columns = {\n",
    "        \"FIPS Code\" : \"FIPS\",\n",
    "        \"State\" : \"State\",\n",
    "        \"Area name\" : \"County\",\n",
    "        \"Less than a high school diploma, 2014-18\" : \"Less_HS\",\n",
    "        \"High school diploma only, 2014-18\" : \"HS_only\",\n",
    "        \"Some college or associate's degree, 2014-18\" : \"College_Associate\",\n",
    "        \"Bachelor's degree or higher, 2014-18\" : \"Bachelors\",\n",
    "        \"Percent of adults with less than a high school diploma, 2014-18\" : \"PCT_Less_HS\",\n",
    "        \"Percent of adults with a high school diploma only, 2014-18\" : \"PCT_HS_only\",\n",
    "        \"Percent of adults completing some college or associate's degree, 2014-18\" : \"PCT_College_Associate\",\n",
    "        \"Percent of adults with a bachelor's degree or higher, 2014-18\" : \"PCT_Bachelors\"\n",
    "    }\n",
    "    data = pd.read_csv(\"./datasets/USDoA/Education_2020_5_7.csv\", sep=\",\", header=0, engine=\"python\", usecols=columns.keys(), dtype=dtype, thousands=\",\").rename(columns=columns)\n",
    "    return filter_counties(data)\n",
    "\n",
    "# retrieves relevant population data\n",
    "def get_population_data():\n",
    "    dtype = {\n",
    "        \"FIPS\" : \"Int32\",\n",
    "        \"CENSUS_2010_POP\" : \"Int32\",\n",
    "        \"POP_ESTIMATE_2018\" : \"Int32\",\n",
    "        \"Births_2018\" : \"Int32\",\n",
    "        \"Deaths_2018\" : \"Int32\",\n",
    "        \"INTERNATIONAL_MIG_2018\" : \"Int32\",\n",
    "        \"DOMESTIC_MIG_2018\" : \"Int32\",\n",
    "        \"NET_MIG_2018\" : \"Int32\",\n",
    "        \"GQ_ESTIMATES_2018\" : \"Int32\"\n",
    "    }\n",
    "    columns = {\n",
    "        \"FIPS\" : \"FIPS\",\n",
    "        \"State\" : \"State\",\n",
    "        \"Area_Name\" : \"County\",\n",
    "        \"CENSUS_2010_POP\" : \"CENSUS_2010_POP\",                     # 4/1/2010 resident total Census 2010 population\n",
    "        \"POP_ESTIMATE_2018\" : \"POP_ESTIMATE_2018\",                 # 7/1/2018 resident total population estimate\n",
    "        \"Births_2018\" : \"BIRTHS_2018\",                             # Births in period 7/1/2017 to 6/30/2018\n",
    "        \"Deaths_2018\" : \"DEATHS_2018\",                             # Deaths in period 7/1/2017 to 6/30/2018\n",
    "        \"INTERNATIONAL_MIG_2018\" : \"INTERNATIONAL_MIG_2018\",       # Net international migration in period 7/1/2017 to 6/30/2018\n",
    "        \"DOMESTIC_MIG_2018\" : \"DOMESTIC_MIG_2018\",                 # Net domestic migration in period 7/1/2017 to 6/30/2018\n",
    "        \"NET_MIG_2018\" : \"NET_MIG_2018\",                           # Net migration in period 7/1/2017 to 6/30/2018\n",
    "        \"GQ_ESTIMATES_2018\" : \"GQ_ESTIMATES_2018\",                 # 7/1/2018 Group Quarters total population estimate\n",
    "        \"R_birth_2018\" : \"R_BIRTHS_2018\",                          # Birth rate in period 7/1/2017 to 6/30/2018 (per thousand people?)\n",
    "        \"R_death_2018\" : \"R_DEATHS_2018\",                          # Death rate in period 7/1/2017 to 6/30/2018 (per thousand people?)\n",
    "        \"R_INTERNATIONAL_MIG_2018\" : \"R_INTERNATIONAL_MIG_2018\",   # Net international migration rate in period 7/1/2017 to 6/30/2018 (per thousand people?)\n",
    "        \"R_DOMESTIC_MIG_2018\" : \"R_DOMESTIC_MIG_2018\",             # Net domestic migration rate in period 7/1/2017 to 6/30/2018 (per thousand people?)\n",
    "        \"R_NET_MIG_2018\" : \"R_NET_MIG_2018\"                        # Net migration rate in period 7/1/2017 to 6/30/2018 (per thousand people?)\n",
    "    }\n",
    "    data = pd.read_csv(\"./datasets/USDoA/PopulationEstimates_2020_5_7.csv\", sep=\",\", header=0, engine=\"python\", usecols=columns.keys(), dtype=dtype, thousands=\",\").rename(columns=columns)\n",
    "    return filter_counties(data)\n",
    "\n",
    "# retrieves relevant poverty data\n",
    "def get_poverty_data():\n",
    "    dtype = {\n",
    "        \"FIPStxt\" : \"Int32\",\n",
    "        \"POVALL_2018\" : \"Int32\",\n",
    "        \"POV017_2018\" : \"Int32\",\n",
    "        \"POV517_2018\" : \"Int32\",\n",
    "        \"MEDHHINC_2018\" : \"Int32\"\n",
    "    }\n",
    "    columns = {\n",
    "        \"FIPStxt\" : \"FIPS\",\n",
    "        \"Stabr\" : \"State\",\n",
    "        \"Area_name\" : \"County\",\n",
    "        \"POVALL_2018\" : \"Poverty_Total\",               # total amount in poverty in 2018\n",
    "        \"PCTPOVALL_2018\" : \"PCT_Poverty_Total\",        # percentage in poverty in 2018\n",
    "        \"POV017_2018\" : \"Poverty_0_17\",                # amount in poverty ages 0-17 in 2018\n",
    "        \"PCTPOV017_2018\" : \"PCT_Poverty_0_17\",         # percentage in poverty ages 0-17 in 2018\n",
    "        \"POV517_2018\" : \"Poverty_5_17\",                # amount in poverty ages 5-17 in 2018\n",
    "        \"PCTPOV517_2018\" : \"PCT_Poverty_5_17\",         # percentage in poverty ages 5-17 in 2018\n",
    "#         \"MEDHHINC_2018\" : \"Median_Household_Income\"    # median household income in 2018    NOTE: repetitive?\n",
    "    }\n",
    "    data = pd.read_csv(\"./datasets/USDoA/PovertyEstimates_2020_5_7.csv\", sep=\",\", header=0, engine=\"python\", usecols=columns.keys(), dtype=dtype, thousands=\",\").rename(columns=columns)\n",
    "    return filter_counties(data)\n",
    "\n",
    "# retrieves relevant unemployment data\n",
    "def get_unemployment_data():\n",
    "    dtype = {\n",
    "        \"FIPS\" : \"Int32\",\n",
    "        \"Civilian_labor_force_2018\" : \"Int32\",\n",
    "        \"Employed_2018\" : \"Int32\",\n",
    "        \"Unemployed_2018\" : \"Int32\",\n",
    "    }\n",
    "    columns = {\n",
    "        \"FIPS\" : \"FIPS\",\n",
    "        \"State\" : \"State\",\n",
    "        \"Area_name\" : \"County\",\n",
    "        \"Civilian_labor_force_2018\" : \"Total_Labor_Force\",\n",
    "        \"Employed_2018\" : \"Employed\",\n",
    "        \"Unemployed_2018\" : \"Unemployed\",\n",
    "        \"Unemployment_rate_2018\" : \"PCT_Unemployed\",\n",
    "        \"Median_Household_Income_2018\" : \"Median_Household_Income\",\n",
    "        \"Med_HH_Income_Percent_of_State_Total_2018\" : \"Median_Household_Income_County_to_State_Ratio\"\n",
    "    }\n",
    "    data = pd.read_csv(\"./datasets/USDoA/Unemployment_2020_5_7.csv\", sep=\",\", header=0, engine=\"python\", usecols=columns.keys(), dtype=dtype, thousands=\",\").rename(columns=columns)\n",
    "    # remove non-counties\n",
    "    data = filter_counties(data)\n",
    "    # strip and convert \"Median_Household_Income\" column to int\n",
    "    data[\"Median_Household_Income\"] = data[\"Median_Household_Income\"].astype(str).map(lambda x: x.lstrip(\"$\").rstrip(\" \").replace(\",\", \"\")).astype(int)\n",
    "    return data\n",
    "    \n",
    "# retrieves relevant land area data\n",
    "def get_land_data():\n",
    "    columns = {\n",
    "        \"Areaname\" : \"Areaname\",       # County, State\n",
    "        \"STCOU\" : \"FIPS\",              # FIPS\n",
    "        \"LND110210D\" : \"Land_Area\",    # -> Land area in square miles 2010\n",
    "        \"LND210200D\" : \"Water_Area\"    # -> Water area in square miles 2000\n",
    "    }\n",
    "    data = pd.read_csv(\"./datasets/USCensus/LND01_2020_5_21.csv\", sep=\",\", header=0, engine=\"python\", usecols=columns.keys()).rename(columns=columns)\n",
    "    return data\n",
    "\n",
    "# retrieves relevant housing data\n",
    "def get_housing_data():\n",
    "    dtype = {\n",
    "        \"STCOU\" : \"Int32\",\n",
    "        \"HSG030210D\" : \"Int32\",\n",
    "        \"HSG171209D\" : \"Int32\",\n",
    "        \"HSG172209D\" : \"Int32\",\n",
    "        \"HSG173209D\" : \"Int32\",\n",
    "        \"HSG174209D\" : \"Int32\",\n",
    "        \"HSG175209D\" : \"Int32\",\n",
    "        \"HSG176209D\" : \"Int32\",\n",
    "        \"HSG177209D\" : \"Int32\",\n",
    "        \"HSG178209D\" : \"Int32\",\n",
    "        \"HSG179209D\" : \"Int32\"\n",
    "    }\n",
    "    columns = {\n",
    "        \"Area_name\" : \"Areaname\",         # County, State\n",
    "        \"STCOU\" : \"FIPS\",                 # FIPS\n",
    "        \"HSG030210D\" : \"Total_Units\",     # Total housing units 2010 (complete count)\n",
    "        \"HSG171209D\" : \"1_Room\",          # Housing units with 1 room, 2005-2009\n",
    "        \"HSG172209D\" : \"2_Rooms\",\t      # Housing units with 2 rooms, 2005-2009\n",
    "        \"HSG173209D\" : \"3_Rooms\",\t      # Housing units with 3 rooms, 2005-2009\n",
    "        \"HSG174209D\" : \"4_Rooms\",\t      # Housing units with 4 rooms, 2005-2009\n",
    "        \"HSG175209D\" : \"5_Rooms\",\t      # Housing units with 5 rooms, 2005-2009\n",
    "        \"HSG176209D\" : \"6_Rooms\",\t      # Housing units with 6 rooms, 2005-2009\n",
    "        \"HSG177209D\" : \"7_Rooms\",\t      # Housing units with 7 rooms, 2005-2009\n",
    "        \"HSG178209D\" : \"8_Rooms\",\t      # Housing units with 8 rooms, 2005-2009\n",
    "        \"HSG179209D\" : \"9+_Rooms\",\t      # Housing units with 9 rooms or more, 2005-2009\n",
    "        \"HSG180209D\" : \"Median_Rooms\"     # Housing units - median rooms, 2005-2009\n",
    "    }\n",
    "    data = pd.read_csv(\"./datasets/USCensus/HSG_modified.csv\", sep=\",\", header=0, engine=\"python\", dtype=dtype, thousands=\",\").rename(columns=columns)\n",
    "    return data\n",
    "\n",
    "# retrieves relevant age data\n",
    "def get_age_data():\n",
    "    dtype = {\n",
    "        \"STCOU\" : \"Int32\",\n",
    "        \"AGE290209D\" : \"Int32\",\n",
    "        \"AGE130209D\" : \"Int32\",\n",
    "        \"AGE160209D\" : \"Int32\",\n",
    "        \"AGE230209D\" : \"Int32\",\n",
    "        \"AGE260209D\" : \"Int32\",\n",
    "        \"AGE340209D\" : \"Int32\",\n",
    "        \"AGE370209D\" : \"Int32\",\n",
    "        \"AGE430209D\" : \"Int32\",\n",
    "        \"AGE460209D\" : \"Int32\",\n",
    "        \"AGE530209D\" : \"Int32\",\n",
    "        \"AGE560209D\" : \"Int32\",\n",
    "        \"AGE630209D\" : \"Int32\",\n",
    "        \"AGE660209D\" : \"Int32\",\n",
    "        \"AGE690209D\" : \"Int32\",\n",
    "        \"AGE730209D\" : \"Int32\",\n",
    "        \"AGE800209D\" : \"Int32\",\n",
    "        \"AGE830209D\" : \"Int32\",\n",
    "        \"AGE870209D\" : \"Int32\",\n",
    "        \"AGE900209D\" : \"Int32\"\n",
    "    }\n",
    "    columns = {\n",
    "        \"Areaname\" : \"Areaname\",              # County, State\n",
    "        \"STCOU\" : \"FIPS\",                     # FIPS\n",
    "        \"AGE050200D\" : \"Median_Age\",          # Resident population: Median age (April 1 - complete count) 2000\n",
    "        \"AGE290209D\" : \"Population_0_18\",     # Resident population under 18 years (July 1 - estimate) 2009\n",
    "        \"AGE130209D\" : \"Population_0_5\",      # Resident population under 5 years (July 1 - estimate) 2009\n",
    "        \"AGE160209D\" : \"Population_5_9\",      # Resident population 5 to 9 years (July 1 - estimate) 2009\n",
    "        \"AGE230209D\" : \"Population_10_14\",    # Resident population 10 to 14 years (July 1 - estimate) 2009\n",
    "        \"AGE260209D\" : \"Population_15_19\",    # Resident population 15 to 19 years (July 1 - estimate) 2009\n",
    "        \"AGE340209D\" : \"Population_20_24\",    # Resident population 20 to 24 years (July 1 - estimate) 2009\n",
    "        \"AGE370209D\" : \"Population_25_29\",    # Resident population 25 to 29 years (July 1 - estimate) 2009\n",
    "        \"AGE430209D\" : \"Population_30_34\",    # Resident population 30 to 34 years (July 1 - estimate) 2009\n",
    "        \"AGE460209D\" : \"Population_35_39\",    # Resident population 35 to 39 years (July 1 - estimate) 2009\n",
    "        \"AGE530209D\" : \"Population_40_44\",    # Resident population 40 to 44 years (July 1 - estimate) 2009\n",
    "        \"AGE560209D\" : \"Population_45_49\",    # Resident population 45 to 49 years (July 1 - estimate) 2009\n",
    "        \"AGE630209D\" : \"Population_50_54\",    # Resident population 50 to 54 years (July 1 - estimate) 2009\n",
    "        \"AGE660209D\" : \"Population_55_59\",    # Resident population 55 to 59 years (July 1 - estimate) 2009\n",
    "        \"AGE690209D\" : \"Population_60_64\",    # Resident population 60 to 64 years (July 1 - estimate) 2009\n",
    "        \"AGE730209D\" : \"Population_65_69\",    # Resident population 65 to 69 years (July 1 - estimate) 2009\n",
    "        \"AGE800209D\" : \"Population_70_74\",    # Resident population 70 to 74 years (July 1 - estimate) 2009\n",
    "        \"AGE830209D\" : \"Population_75_79\",    # Resident population 75 to 79 years (July 1 - estimate) 2009\n",
    "        \"AGE870209D\" : \"Population_80_84\",    # Resident population 80 to 84 years (July 1 - estimate) 2009\n",
    "        \"AGE900209D\" : \"Population_85+\"       # Resident population 85 years and over (July 1 - estimate) 2009\n",
    "    }\n",
    "    data = pd.read_csv(\"./datasets/USCensus/AGE_modified.csv\", sep=\",\", header=0, engine=\"python\", dtype=dtype, thousands=\",\").rename(columns=columns)\n",
    "    # make PCT columns for relative population diversity\n",
    "    # 0. list columns from age groups 0 to 85+\n",
    "#     population_columns = [\"Population_0_5\", \"Population_5_9\", \"Population_10_14\", \"Population_15_19\", \"Population_20_24\", \"Population_25_29\", \"Population_30_34\",\\\n",
    "#                \"Population_35_39\", \"Population_40_44\", \"Population_45_49\", \"Population_50_54\", \"Population_55_59\", \"Population_60_64\", \"Population_65_69\",\\\n",
    "#                \"Population_70_74\", \"Population_75_79\", \"Population_80_84\", \"Population_85+\"]\n",
    "    population_columns = list(columns.values())[4:]\n",
    "    # 1. get total calculated population for each county\n",
    "    total_population = data[population_columns].sum(axis=\"columns\")\n",
    "    # 2. PCT = age group population / total population\n",
    "    population_columns.append(\"Population_0_18\")\n",
    "    for column in population_columns:\n",
    "        data[\"PCT_\" + column] = data[column] / total_population\n",
    "    \n",
    "    return data\n",
    "\n",
    "get_age_data()\n",
    "\n",
    "# retrieves relevant race data\n",
    "def get_race_data():\n",
    "    dtype = {\n",
    "        \"STCOU\" : \"Int32\",\n",
    "        \"RHI100210D\" : \"Int32\",\n",
    "        \"RHI200210D\" : \"Int32\",\n",
    "        \"RHI300210D\" : \"Int32\",\n",
    "        \"RHI400210D\" : \"Int32\",\n",
    "        \"RHI500210D\" : \"Int32\",\n",
    "        \"RHI600210D\" : \"Int32\",\n",
    "        \"RHI700210D\" : \"Int32\",\n",
    "        \"RHI800210D\" : \"Int32\"\n",
    "    }\n",
    "    columns = {\n",
    "        \"Areaname\" : \"Areaname\",                               # County, State\n",
    "        \"STCOU\" : \"FIPS\",                                      # FIPS\n",
    "        \"RHI100210D\" : \"White_Alone\",                          # Resident population: White alone (April 1 - complete count) 2010\n",
    "        \"RHI200210D\" : \"Black_Alone\",                          # Resident population: Black alone (April 1 - complete count) 2010\n",
    "        \"RHI300210D\" : \"AmericanIndian_AlaskaNative_Alone\",    # Resident population: American Indian and Alaska Native alone (April 1 - complete count) 2010\n",
    "        \"RHI400210D\" : \"Asian_Alone\",                          # Resident population: Asian alone (April 1 - complete count) 2010\n",
    "        \"RHI500210D\" : \"PacificIslander_Alone\",                # Resident population: Native Hawaiian and Other Pacific Islander alone (April 1 - complete count) 2010\n",
    "        \"RHI600210D\" : \"2+_Races\",                             # Resident population: Two or more races (April 1 - complete count) 2010\n",
    "        \"RHI700210D\" : \"Hispanic_Latino_Origin\",               # Resident population: Hispanic or Latino Origin (April 1 - complete count) 2010\n",
    "        \"RHI800210D\" : \"Not_Hispanic_White_Alone\"               # Resident population: Not Hispanic, White alone (April 1 - complete count)  2010\n",
    "    }\n",
    "    data = pd.read_csv(\"./datasets/USCensus/RHI_modified.csv\", sep=\",\", header=0, engine=\"python\", dtype=dtype, thousands=\",\").rename(columns=columns)\n",
    "    # make PCT columns for relative race diversity\n",
    "    # 0. list columns for all races\n",
    "    race_columns = list(columns.values())[2:]\n",
    "    # 1. get total calculated population for each county\n",
    "    total_population = data[race_columns].sum(axis=\"columns\")\n",
    "    # 2. PCT = race population / total population\n",
    "    for column in race_columns:\n",
    "        data[\"PCT_\" + column] = data[column] / total_population\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NYTimes\n",
    "virus_data = get_virus_data()\n",
    "\n",
    "# USCensus data is completely organized, so simply combine the datasets\n",
    "# merges all USCensus data into one dataframe\n",
    "def init_census_data():\n",
    "    df_list = list()\n",
    "    df_list.append(get_land_data())\n",
    "    df_list.append(get_housing_data())\n",
    "    df_list.append(get_age_data())\n",
    "    df_list.append(get_race_data())\n",
    "    \n",
    "    # merge data\n",
    "    data = df_list[0]\n",
    "    for i in range(1, len(df_list)):\n",
    "        # ensure counties are in identical order\n",
    "        if not (data[\"FIPS\"] == df_list[i][\"FIPS\"]).all():\n",
    "            print(\"County order mismatch\")\n",
    "            return None\n",
    "        \n",
    "        # if matches, remove \"FIPS\" and \"Areaname\" columns from all but the first\n",
    "        df_list[i].drop(columns=[\"FIPS\", \"Areaname\"], inplace=True)\n",
    "        \n",
    "        # merge into \"data\" dataframe\n",
    "        data = pd.concat([data, df_list[i]], axis=\"columns\")\n",
    "    \n",
    "    # split county and state names into separate columns\n",
    "    new = data[\"Areaname\"].str.split(pat=\", \", n=1, expand=True)\n",
    "    data[\"County\"] = new[0].apply(lambda s: s + \" County\" if s not in EXCEPTIONS else s)  # add \" County\" if county\n",
    "    data[\"State\"] = new[1]\n",
    "    data.drop(columns=[\"Areaname\"], inplace=True)\n",
    "    \n",
    "    # remove state rows (i.e. rows that describe the entire state)\n",
    "    # aka rows where the \"State\" column is now null due to split()\n",
    "    data = filter_by_value(data, filter_dict={\"County\": EXCEPTIONS}, remove_dict={\"State\": \"NaN\"})\n",
    "    return data\n",
    "\n",
    "# USDoA data is **almost** entirely organized, so combine those together\n",
    "# merges all USDoA data into one dataframe\n",
    "def init_DoA_data():\n",
    "    df_list = list()\n",
    "    df_list.append(get_education_data())\n",
    "    df_list.append(get_population_data())\n",
    "    df_list.append(get_poverty_data())\n",
    "    df_list.append(get_unemployment_data())\n",
    "    \n",
    "    # find any counties that don't exist in all datasets\n",
    "    data = df_list[0]\n",
    "    FIPS_keep = set(data[\"FIPS\"].tolist())   # intersecting FIPS\n",
    "    FIPS_remove = set()                      # symmetric difference FIPS\n",
    "    for i in range(1, len(df_list)):\n",
    "        FIPS_remove = FIPS_remove.union(FIPS_keep ^ set(df_list[i][\"FIPS\"].tolist()))  # FIPS that don't exist in both\n",
    "        FIPS_keep = FIPS_keep.intersection(set(df_list[i][\"FIPS\"].tolist()))           # FIPS that exist in both\n",
    "    # remove the counties that don't exist in all datasets\n",
    "    for i in range(len(df_list)):\n",
    "        i_remove = set()\n",
    "        for fips in FIPS_remove:\n",
    "            i_remove = i_remove.union(set(df_list[i][df_list[i][\"FIPS\"] == fips].index))\n",
    "        df_list[i].drop(index=i_remove, inplace=True)\n",
    "    \n",
    "    # merge data\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "    for i in range(1, len(df_list)):\n",
    "        df_list[i].reset_index(drop=True, inplace=True)\n",
    "        # ensure counties are in identical order\n",
    "        if not (data[\"FIPS\"] == df_list[i][\"FIPS\"]).all():\n",
    "            print(\"County order mismatch\")\n",
    "            return None\n",
    "        \n",
    "        # remove \"State\", \"County\", and \"FIPS\" columns from all but first\n",
    "        df_list[i].drop(columns=[\"State\", \"County\", \"FIPS\"], inplace=True)\n",
    "        \n",
    "        # merge into \"data\" dataframe\n",
    "        data = pd.concat([data, df_list[i]], axis=\"columns\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "census_data = init_census_data()\n",
    "doa_data = init_DoA_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merges all data together based on their respective counties\n",
    "def init_attributes():\n",
    "    df_list = list()\n",
    "    df_list.append(init_DoA_data())\n",
    "    df_list.append(init_census_data())\n",
    "    \n",
    "    # find any counties that don't exist in all datasets\n",
    "    data = df_list[0]\n",
    "    FIPS_keep = set(data[\"FIPS\"].tolist())   # intersecting FIPS\n",
    "    FIPS_remove = set()                      # symmetric difference FIPS\n",
    "    for i in range(1, len(df_list)):\n",
    "        FIPS_remove = FIPS_remove.union(FIPS_keep ^ set(df_list[i][\"FIPS\"].tolist()))  # FIPS that don't exist in both\n",
    "        FIPS_keep = FIPS_keep.intersection(set(df_list[i][\"FIPS\"].tolist()))           # FIPS that exist in both\n",
    "    # remove the counties that don't exist in all datasets\n",
    "    for i in range(len(df_list)):\n",
    "        i_remove = set()\n",
    "        for fips in FIPS_remove:\n",
    "            i_remove = i_remove.union(set(df_list[i][df_list[i][\"FIPS\"] == fips].index))\n",
    "        df_list[i].drop(index=i_remove, inplace=True)\n",
    "        \n",
    "    # merge data\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "    for i in range(1, len(df_list)):\n",
    "        df_list[i].reset_index(drop=True, inplace=True)\n",
    "        # ensure counties are in identical order\n",
    "        if not (data[\"FIPS\"] == df_list[i][\"FIPS\"]).all():\n",
    "            print(\"County order mismatch\")\n",
    "            return None\n",
    "        \n",
    "        # remove \"State\", \"County\", and \"FIPS\" columns from all but first\n",
    "        df_list[i].drop(columns=[\"State\", \"County\", \"FIPS\"], inplace=True)\n",
    "        \n",
    "        # merge into \"data\" dataframe\n",
    "        data = pd.concat([data, df_list[i]], axis=\"columns\")\n",
    "    \n",
    "    return data\n",
    "    \n",
    "attributes_data = init_attributes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: As far as I can tell, the COVID19 data from NYTimes seems to consolidate 5 counties under New York City (and thus it has no listed FIPS code)\n",
    "#       as a result, I will have to simply link the 5 counties (Manhattan/New York, Queens, Bronx, Brooklyn/Kings, Staten Island/Richmond).\n",
    "#       For more information: https://github.com/nytimes/covid-19-data/issues/105\n",
    "# get FIPS codes corresponding to the 5 NYC counties\n",
    "nyc_FIPS = [int(fips) for fips in list(filter_by_value(attributes_data, filter_dict={\"State\": \"NY\", \"County\": [\"Bronx County\", \"Kings County\", \"New York County\", \"Queens County\", \"Richmond County\"]})[\"FIPS\"])]\n",
    "print(nyc_FIPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combines all given records into one (i.e. absolute values can be summed, but percentages and rates need to be recalculated individually)\n",
    "# precondition: for all percentage and rate columns (prefix \"PCT_\" and \"R_\" respectively), there is a corresponding absolute value column of the same name without the prefix\n",
    "def combine_counties(data, combined_FIPS: list = [], fill_data: dict = {}):\n",
    "    if len(combined_FIPS) == 0:\n",
    "        return data\n",
    "    \n",
    "    # ensure necessary data exists in the dataframe\n",
    "    relevant_data = filter_by_value(data, filter_dict={\"FIPS\": combined_FIPS})\n",
    "    if relevant_data.empty:\n",
    "        return data\n",
    "    \n",
    "    # ensure records that match fill_data don't already exist\n",
    "    if not filter_by_value(data, filter_dict=fill_data).empty:\n",
    "        return data\n",
    "    \n",
    "    dtypes = dict(data.dtypes)    # save old types to reconvert new dataframe (they all get casted to float for some reason)\n",
    "    combined = fill_data\n",
    "    # for each column, sum if is absolute value, recalculate if is percentage or rate\n",
    "    for column in data.columns:\n",
    "        # skip the columns already filled\n",
    "        if column in combined.keys():\n",
    "            continue\n",
    "        \n",
    "        # if column is a rate\n",
    "        if column[0:2] == \"R_\":\n",
    "            # rates are defined as value per thousand people per year\n",
    "            # i.e. a 10 birth rate is 10 births per 1000 people per year\n",
    "            # i.e. a 10 birth rate in a 10000 person population is 10 * 10000 / 1000 = 100 births per year\n",
    "            # use \"POP_ESTIMATE_2018\" column as given population to calculate combined rates\n",
    "            # new birth rate = sum(births) * 1000 / sum(population)\n",
    "            sum_subtotal = relevant_data[column.split(\"R_\")[1]].sum(axis=\"index\")\n",
    "            combined[column] = sum_subtotal * 1000 / relevant_data[\"POP_ESTIMATE_2018\"].sum(axis=\"index\")\n",
    "            dtypes[column] = \"float64\"    # ensure float type\n",
    "        # if column is a percentage\n",
    "        elif column[0:4] == \"PCT_\":\n",
    "            # new PCT = sum(subtotal) / sum(total)\n",
    "            # sum(total) = sum(subtotal / subPCT)\n",
    "            sum_subtotal = relevant_data[column.split(\"PCT_\")[1]].sum(axis=\"index\")\n",
    "            sum_total = (relevant_data[column.split(\"PCT_\")[1]] / relevant_data[column]).sum(axis=\"index\")\n",
    "            combined[column] = sum_subtotal / sum_total\n",
    "            dtypes[column] = \"float64\"    # ensure float type\n",
    "        # if column is an absolute value\n",
    "        else:\n",
    "            combined[column] = relevant_data[column].sum(axis=\"index\")\n",
    "    \n",
    "    data = data.append(combined, ignore_index=True)\n",
    "    # convert back to original types\n",
    "    for column in data.columns:\n",
    "        data[column] = data[column].astype(dtypes[column])\n",
    "    \n",
    "    return data.sort_values(by=\"FIPS\", axis=\"index\")\n",
    "\n",
    "# group the 5 NYC counties\n",
    "attributes_data = combine_counties(attributes_data, combined_FIPS=nyc_FIPS, fill_data={\"FIPS\": 36000, \"State\": \"NY\", \"County\": \"NYC\"})\n",
    "# delete the 5 individual counties\n",
    "attributes_data = filter_by_value(attributes_data, remove_dict={\"FIPS\": nyc_FIPS})\n",
    "# remove other counties with unknown FIPS from virus data EXCEPT for NYC\n",
    "virus_data = filter_by_value(virus_data, filter_dict={\"County\": \"New York City\"}, remove_dict={\"FIPS\": -1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change NYC FIPS to 36000 in data if necessary\n",
    "def apply_FIPS(data, fips: int = -1, condition_dict: dict = {}):\n",
    "    df = data.copy()\n",
    "    for col, value in condition_dict.items():\n",
    "        df.loc[df[col] == value, \"FIPS\"] = fips\n",
    "    return df\n",
    "        \n",
    "virus_data = apply_FIPS(virus_data, fips=36000, condition_dict={\"County\": \"New York City\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "virus_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge census/DoA data into virus data\n",
    "def merge_data(a_data, v_data):\n",
    "    data = a_data.copy()\n",
    "    virus_data = v_data.copy()\n",
    "    dtypes = dict(data.dtypes)\n",
    "    dtypes.update(dict(virus_data.dtypes))\n",
    "    \n",
    "    # 1. remove \"County\" and \"State\" columns from virus_data to avoid repeats\n",
    "    virus_data.drop(columns=[\"County\", \"State\"], inplace=True)\n",
    "    \n",
    "    # 2. remove counties that don't exist in both datasets\n",
    "    data_remove_index = set()\n",
    "    virus_remove_index = set()\n",
    "    # for fips that don't exist in both datasets:\n",
    "    for fips in set(data[\"FIPS\"]) ^ set(virus_data[\"FIPS\"]):\n",
    "        data_remove_index = data_remove_index.union(data[data[\"FIPS\"] == fips].index)\n",
    "        virus_remove_index = virus_remove_index.union(virus_data[virus_data[\"FIPS\"] == fips].index)\n",
    "    data.drop(index=data_remove_index, inplace=True)\n",
    "    virus_data.drop(index=virus_remove_index, inplace=True)\n",
    "    \n",
    "    # 3. extend each virus record with attributes data, then append to list\n",
    "    print(\"Starting...\")\n",
    "    start_time = time.time()\n",
    "    cycle_time = start_time\n",
    "    combined_list = []\n",
    "    iterations = 0\n",
    "    cycles = 0\n",
    "    cycle_count = 50000\n",
    "    for fips in set(virus_data[\"FIPS\"]):            \n",
    "        attributes = data[data[\"FIPS\"] == fips]\n",
    "        if attributes.empty:\n",
    "            print(\"Attributes for county {} not found, exiting...\".format(fips))\n",
    "            break\n",
    "        attributes = attributes.drop(columns=\"FIPS\").iloc[0]\n",
    "        virus_records_by_FIPS = virus_data[virus_data[\"FIPS\"] == fips]\n",
    "        # add same attributes to all entries of this county\n",
    "        attributes_df = pd.DataFrame([attributes for _ in range(len(virus_records_by_FIPS))], index=virus_records_by_FIPS.index)\n",
    "        new_records = pd.concat([virus_records_by_FIPS, attributes_df], axis=\"columns\")        \n",
    "        combined_list.append(new_records)\n",
    "        \n",
    "        iterations += len(virus_records_by_FIPS)\n",
    "        if iterations > cycle_count:\n",
    "            prev_time = cycle_time\n",
    "            cycle_time = time.time()\n",
    "            print(\"Iteration {} in {} seconds\".format(cycles*cycle_count + iterations, cycle_time - prev_time))\n",
    "            iterations -= cycle_count\n",
    "            cycles += 1\n",
    "    print(\"{} records extended in {} seconds\".format(cycles*cycle_count + iterations, time.time() - start_time))\n",
    "    print(\"{} counties kept\".format(len(set(virus_data[\"FIPS\"]))))\n",
    "    \n",
    "    # 4. turn list of dataframes into one dataframe\n",
    "    df_time = time.time()\n",
    "    combined = pd.DataFrame(pd.concat(combined_list, axis=\"index\"))\n",
    "    print(\"DataFrame created in {} seconds\".format(time.time() - df_time))\n",
    "    \n",
    "    # 5. convert back to original types\n",
    "    for column in combined.columns:\n",
    "        combined[column] = combined[column].astype(dtypes[column])\n",
    "    \n",
    "    return combined.sort_index()\n",
    "\n",
    "data = merge_data(attributes_data, virus_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data[\"County\"] == \"San Mateo County\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data based on \"by\" parameter, returns dictionary of dataframes split by key\n",
    "def split_data(data, by: str) -> dict:\n",
    "    if not by in data.columns:\n",
    "        print(\"Column not found, exiting...\")\n",
    "        return None\n",
    "    \n",
    "    split_dict = dict()\n",
    "    for value in set(data[by]):\n",
    "        split_dict[value] = data[data[by] == value]\n",
    "    return split_dict\n",
    "    \n",
    "data_by_state = split_data(data, by=\"State\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(data_by_state[\"HI\"][\"FIPS\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into features and target(s)\n",
    "def feature_target_split(df, targets: list, drops: list = list()):\n",
    "    if len(targets) == 0:\n",
    "        print(\"No target specified, exiting...\")\n",
    "        return None\n",
    "    \n",
    "    data = df.copy()\n",
    "    \n",
    "    # ensure all columns to be dropped exist\n",
    "    for drop in drops:\n",
    "        if not drop in data.columns:\n",
    "            print(\"Column {} not found, can't be dropped, exiting...\".format(drop))\n",
    "            return None\n",
    "    # drop specified columns\n",
    "    data.drop(columns=drops, inplace=True)\n",
    "    \n",
    "    # separate targets\n",
    "    targets_list = list()\n",
    "    for target in targets:\n",
    "        # ensure all targets exist\n",
    "        if not target in data.columns:\n",
    "            print(\"Target {} not found, exiting...\".format(target))\n",
    "            return None\n",
    "        targets_list.append(data[target])\n",
    "        \n",
    "    # separate features\n",
    "    features = data.drop(columns=targets)\n",
    "    # if just one target, return by itself\n",
    "    if len(targets_list) == 1:\n",
    "        return features, targets_list[0]\n",
    "    # if multiple targets, return as list\n",
    "    return features, targets_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize columns, omitting any specified columns\n",
    "def normalize_columns(data, ignore: list = []):\n",
    "    normalized = data.drop(columns=ignore)\n",
    "    means = normalized.mean(axis=\"index\", skipna=False)\n",
    "    stds = normalized.std(axis=\"index\", skipna=False)\n",
    "    for column in normalized.columns:\n",
    "        normalized[column] = normalized[column].sub(means[column]).div(stds[column])\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print weight values with their labels\n",
    "# if top specified, print highest \"top\" weights; if not, print all\n",
    "def output_weights(labels, weights, top: int = 0, console: bool = False):\n",
    "    if len(labels) != len(weights):\n",
    "        print(\"Length of labels and weights don't match, exiting...\")\n",
    "        return None\n",
    "    \n",
    "    if console: print(\"     -----\")\n",
    "    sorted_weights = sorted(zip(abs(weights), labels, list(range(len(weights)))), reverse=True)        \n",
    "    if top <= 0:\n",
    "        for i in range(len(sorted_weights)):\n",
    "            if console: print(\"{}: {}\".format(sorted_weights[i][1], weights[sorted_weights[i][2]]))\n",
    "            top_list.append((sorted_weights[i][1], weights[sorted_weights[i][2]]))\n",
    "    else:\n",
    "        if top > len(weights):\n",
    "            top = len(weights)\n",
    "        \n",
    "        top_list = list()\n",
    "        if console: print(\"Top {} weights:\".format(top))\n",
    "        for i in range(top):\n",
    "            if console: print(\"{}: {}\".format(sorted_weights[i][1], weights[sorted_weights[i][2]]))\n",
    "            top_list.append((sorted_weights[i][1], weights[sorted_weights[i][2]]))\n",
    "    if console: print(\"     -----\")\n",
    "    return top_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performs linear regression on data\n",
    "# cycle_duration: split data off into sections spanning cycle_duration (in days) and fit model\n",
    "# regression_type: False -> linear, True -> ridge\n",
    "def regression(data, regression_type: bool = False, filename: str = \"\", cycle_duration: int = 7, start_day: int = -1, top: int = 10, console: bool = False):\n",
    "    if type(data) is dict:\n",
    "        # parallel arrays to write data to csv file\n",
    "        categories = list()\n",
    "        train_cases_scores = list()\n",
    "        train_deaths_scores = list()\n",
    "        test_cases_scores = list()\n",
    "        test_deaths_scores = list()\n",
    "        top_cases_weights = list()\n",
    "        top_deaths_weights = list()\n",
    "        records_used = list()\n",
    "        counties_used = list()\n",
    "        \n",
    "        start_time = time.time()\n",
    "        for category in sorted(data.keys()):\n",
    "            # TODO: add number of different counties used\n",
    "            categories.append(category)\n",
    "            df = data[category]\n",
    "            if console:\n",
    "                print(\"START: {}\".format(category))\n",
    "                print(\"--------------------\")\n",
    "            # set starting day of first cycle\n",
    "            max_day = max(df[\"Day\"])\n",
    "            min_day = max(min(df[\"Day\"]), start_day)\n",
    "            start_day = ((max_day + 1 - min_day) % cycle_duration) + min_day\n",
    "            # list of dataframes split into their respective cycles\n",
    "            df_cycles = [filter_by_value(df, filter_dict={\"Day\": list(range(day, day + cycle_duration))}) for day in range(start_day, max_day+1, cycle_duration)]\n",
    "            \n",
    "            for cycle, df_cycle in enumerate(df_cycles):\n",
    "                if console:\n",
    "                    print(\"   -----\")\n",
    "                    print(\"Cycle {}: [Days {}~{}]\".format(cycle, min(df_cycle[\"Day\"]), min(df_cycle[\"Day\"])+cycle_duration-1))\n",
    "                X, [Y_Cases, Y_Deaths] = feature_target_split(df_cycle, targets=[\"Cases\", \"Deaths\"])\n",
    "                X = normalize_columns(X, ignore=[\"Date\", \"FIPS\", \"State\", \"County\"])\n",
    "                X_train, X_test, Y_train_Cases, Y_test_Cases, Y_train_Deaths, Y_test_Deaths = \\\n",
    "                    train_test_split(X, Y_Cases, Y_Deaths, test_size=0.2, random_state=42)\n",
    "                \n",
    "                if regression_type:\n",
    "                    reg_Cases = Ridge().fit(X_train, Y_train_Cases)\n",
    "                    reg_Deaths = Ridge().fit(X_train, Y_train_Deaths)\n",
    "                else:\n",
    "                    reg_Cases = LinearRegression().fit(X_train, Y_train_Cases)\n",
    "                    reg_Deaths = LinearRegression().fit(X_train, Y_train_Deaths)\n",
    "                \n",
    "                train_cases_scores.append(reg_Cases.score(X_train, Y_train_Cases))\n",
    "                train_deaths_scores.append(reg_Deaths.score(X_train, Y_train_Deaths))\n",
    "                test_cases_scores.append(reg_Cases.score(X_test, Y_test_Cases))\n",
    "                test_deaths_scores.append(reg_Deaths.score(X_test, Y_test_Deaths))\n",
    "                if console:\n",
    "                    print(\"Training score (cases): {}\".format(train_cases_scores[-1]))\n",
    "                    print(\"Training score (deaths): {}\".format(train_deaths_scores[-1]))\n",
    "                    print(\"Test score (cases): {}\".format(test_cases_scores[-1]))\n",
    "                    print(\"Test score (deaths): {}\".format(test_deaths_scores[-1]))\n",
    "                top_cases_weights.append(output_weights(X.columns, reg_Cases.coef_, top, console))\n",
    "                top_deaths_weights.append(output_weights(X.columns, reg_Deaths.coef_, top, console))\n",
    "                records_used.append(len(df_cycle))\n",
    "                counties_used.append(len(set(df_cycle[\"FIPS\"])))\n",
    "                if console: print(\"   -----\")\n",
    "            if console: print(\"--------------------\")\n",
    "        print(\"Finished in {} seconds\".format(time.time() - start_time))\n",
    "        if len(filename) > 0:\n",
    "            with open(\"./outputs/\" + filename, mode=\"w\", newline=\"\") as logs:\n",
    "                logger = csv.writer(logs, delimiter=\",\")\n",
    "                logger.writerow([\"CATEGORY\"] + categories)\n",
    "                logger.writerow([\"TRAIN CASES SCORES\"] + train_cases_scores)\n",
    "                logger.writerow([\"TRAIN DEATHS SCORES\"] + train_deaths_scores)\n",
    "                logger.writerow([\"TEST CASES SCORES\"] + test_cases_scores)\n",
    "                logger.writerow([\"TEST DEATHS SCORES\"] + test_deaths_scores)\n",
    "                # TODO: this is kind of ugly, rewrite if possible\n",
    "                for num in range(top):\n",
    "                    top_num_row = list()\n",
    "                    for cat in range(len(categories)):\n",
    "                        top_num_row.append(top_cases_weights[cat][num])\n",
    "                    logger.writerow([\"TOP CASES WEIGHT #{}\".format(num+1)] + top_num_row)\n",
    "                for num in range(top):\n",
    "                    top_num_row = list()\n",
    "                    for cat in range(len(categories)):\n",
    "                        top_num_row.append(top_deaths_weights[cat][num])\n",
    "                    logger.writerow([\"TOP DEATHS WEIGHT #{}\".format(num+1)] + top_num_row)\n",
    "                logger.writerow([\"Records Used\"] + records_used)\n",
    "                logger.writerow([\"Counties Used\"] + counties_used)\n",
    "                \n",
    "    # if not dictionary\n",
    "    else:\n",
    "#         print(\"Not implemented yet! Exiting...\")\n",
    "        return regression({\"US\": data}, regression_type, filename, cycle_duration, start_day, top, console)\n",
    "        \n",
    "    return (categories, train_cases_scores, train_deaths_scores, test_cases_scores, test_deaths_scores, top_cases_weights, top_deaths_weights)\n",
    "\n",
    "regression(data_by_state, regression_type=False, filename=\"linear_regression_by_state.csv\", cycle_duration=30, start_day=100)\n",
    "regression(data, regression_type=False, filename=\"linear_regression.csv\", cycle_duration=30, start_day=100)\n",
    "regression(data_by_state, regression_type=True, filename=\"ridge_regression_by_state.csv\", cycle_duration=30, start_day=100)\n",
    "regression(data, regression_type=True, filename=\"ridge_regression.csv\", cycle_duration=30, start_day=100)\n",
    "regression(data, regression_type=False, filename=\"linear_regression_1_day.csv\", cycle_duration=2, start_day=130, console=True)\n",
    "regression(data, regression_type=True, filename=\"ridge_regression_1_day.csv\", cycle_duration=2, start_day=130, console=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
